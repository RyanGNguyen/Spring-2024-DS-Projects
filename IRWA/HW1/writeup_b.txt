My approach was simple. First, I considered whether to use different classifiers
based off of other models available on the SciKit-Learn documentation. From my research,
the most common other models on SciKit-Learn for text classification that supports 
multiple label output for a single target was Logistic Regression, K Neighbors Classifier,
Multinomial Naive Bayes, LinearSVC & SVC, Random Forest Classifier, Gradient Boosting Classifier,
& MLP Classifier. I tested all of these using my final feature list shown below while using
the standard DecisionTreeClassifier as a baseline, and Gradient Boosting Classifier performed 
the best.

Here is how I chose my features. First, I ran the model using only len(text), len(text.strip()),
and len(words) as features as a baseline. Then, I progressively added features, starting 
with the features already in the template code first, and accepted/rejected them based on 
whether I saw significant (>0.02) improvements to the precision, recall, & f1-scores of 
any of the labels as well as the overall accuracy. I initially focused on the number of all
the keyboard symbols within each line of text, and then I expanded to counting the number of 
words that fit various qualifiers (e.g. # of pure alphabetical words, # of numerics, etc.). 
One trend I noticed was how headers often had uppercase words or only the first letter 
capitalized and followed by a colon, so I added this as an explicit feature. The culmination of 
these features led to a satisfactory performance on the training data. On the most recent run, 
the accuracy (rounded to 2 decimal places) was 0.91 by line and 0.85 by segment. Graphics and 
headers had 1.00 for all metrics. By line, tables had a precision of 0.99, recall of 0.95, and 
f1-score of 0.97. Plain text and quotes had scores all around 0.9. Signatures significantly 
lagged in performance at around 0.75 f1-score by line and 0.80 f1-score by segment. However, the 
notes mentioned that signatures were notoriously difficult to distinguish, so this is somewhat 
acceptable. 